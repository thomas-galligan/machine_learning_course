{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Introduction to Machine Learning\n",
    "### Tom Galligan (thomas.galligan@bnc.ox.ac.uk)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the Oxford University CodeSoc Introduction to Machine Learning course! By the end of this course, you'll have been exposed to the central ideas of machine learning (ML), and will have worked with some of its most popular and powerful algorithms. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing you need to do is download the relevant software packages. \n",
    "In this course, you'll need Python 3.5 with numpy, pandas, matplotlib and scikit learn (you might need to downgrade if you have a more recent version). You can download Python 3.5 from https://www.python.org/downloads/. \n",
    "\n",
    "I recommend installing Anaconda (https://docs.anaconda.com/anaconda/install/). This has all the main Python packages you'll need. You can also use pip, which is a bit more light-weight.\n",
    "\n",
    "Then in a terminal, type the following:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install ipython\n",
    "conda install keras\n",
    "conda install tensorflow\n",
    "conda install sklearn\n",
    "conda install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type these in one at a time.\n",
    "If you have any of these packages already, you can do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda update ipython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "To help things run more smoothly, please have these packages installed before the first class so we can get straight into coding.\n",
    "\n",
    "Prerequisites for the course:\n",
    "* Basic familiarity with Python will help a lot, but complete beginners should still find it enjoyable!\n",
    "* Some knowledge of basic maths and statistics (mean, variance, notion of a function) will help a lot. \n",
    "* The deeper theory won't be covered in the classes, but if you want to look into it you'll find it easier if you're comfortable with things like partial derivatives, n-dimensional Gaussians, linear algebra, and metrics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1: k-Nearest Neighbors\n",
    "\n",
    "The first thing we'll do is open up an iPython shell. To do this, open up terminal/command prompt and type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipython --pylab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This option --pylab will start iPython with numpy and matplotlib already imported. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to install the other packages we need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn import neighbors, datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: sklearn changed their modules in a recent version, so a lot of the examples on the web are out of date. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to load the data for our example. To do this, type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y  = datasets.load_iris(return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X is a table containing the features in the following columns: sepal length, sepal width, petal length, petal width. y contains the species of each flower (setosa (0), versicolor (1), virginica (2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all let's plot the data to see what we're working with (this almost always a good place to start)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:50,0],X[:50,1],color='blue',label='setosa')\n",
    "plt.scatter(X[50:100,0],X[50:100,1],color='red',label='versicolor')\n",
    "plt.scatter(X[100:150,0],X[100:150,1],color='black',label='virginica')\n",
    "plt.legend()\n",
    "plt.xlabel('sepal length')\n",
    "plt.ylabel('sepal width')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that it should be easy to identify setosa purely from the sepal length and width, but to identify between versicolor and virginica, we'll need to include more data. Close the previous plot, and type the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:50,2],X[:50,3],color='blue',label='setosa')\n",
    "plt.scatter(X[50:100,2],X[50:100,3],color='red',label='versicolor')\n",
    "plt.scatter(X[100:150,2],X[100:150,3],color='black',label='virginica')\n",
    "plt.legend()\n",
    "plt.xlabel('petal length')\n",
    "plt.ylabel('petal width')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we should be able to build a good classifying model from our data! First of all, we need to separate our data into a training batch and a test batch. This makes sure our test is 'clean', and is especially important in models where the training process is more involved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train) # fit only the training data\n",
    "X_train_std = scaler.transform(X_train)\n",
    "X_test_std = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll initiate our model, considering the 3 nearest neighbors for now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "knn = KNeighborsClassifier(n_neighbors=k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And train it on our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This might seem like a big step with a lot going on behind the scenes. In reality, all we've done is load the training data into the model. This is because kNN is a 'lazy-learner'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to evaluate the accuracy of our model. Let's get it to take the features of the test data, and for each flower predict its species. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = knn.predict(X_test_std)\n",
    "print(accuracy_score(y_test,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm predicts the species to 98% accuracy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "In the algorithm we created, we picked an arbitrary value of k. Depending on our value of k, we might get algorithms with different accuracy. Let's explore this further. \n",
    "\n",
    "We need a way to determine how the algorithm's accuracy compares as we tune the hyperparameter k. First of all, let's plot the accuracy for a range of values of k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_scores = []\n",
    "for k in range(1,51):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X_train, y_train, cv = 10, scoring='accuracy')\n",
    "    k_scores.append(scores.mean())\n",
    "\n",
    "plt.plot(np.arange(1,51), k_scores)\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should find that the most accurate model is given by $k=7$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/cross_val.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge: Boston House Prices\n",
    "\n",
    "First, load the data, and switch from a classifier to a regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "X, y = load_boston(return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a KNN model to predict the housing price as accurately as possible! Since we're now performing a regression rather than a classification, you'll need to think about how we can judge the accuracy of our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ________________________________________________________________________\n",
    "\n",
    "# Chapter 2: Random Forests\n",
    "\n",
    "\n",
    "We want to build a random forest regression algorithm to predict Boston house prices more accurately. To do this, let's first import the modules we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the rest of the process should look familiar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_boston(return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forests don't need to have the data scaled first, so no need for a StandardScaler() step!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "rfr = RandomForestRegressor(n_estimators = 50, random_state = 42)\n",
    "rfr.fit(X_train, y_train) #Â this step builds the random forest\n",
    "y_pred = rfr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how good our random forest is. Remember the KNN gave a MSE of ~ 37.7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time our MSE is much lower than for the K Nearest Neighbors algorithm! \n",
    "And we've not even done any cross validation yet.\n",
    "##### The random forest is in general a much better choice for high-dimensional feature spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/knn_vs_rf.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importances\n",
    "\n",
    "We can examine our feature importances to better understand our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = rfr.feature_importances_\n",
    "indices = np.argsort(importances)[::-1] \n",
    "for f in range(X.shape[1]): \n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation of Random Forests\n",
    "\n",
    "If you look at the sklearn documentation on random forests, you'll see that we have a huge number of options when we're building a regression model.\n",
    "\n",
    "<img src='img/rfr_documentation.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll just do the same thing, but now we're going to be searching through a hyperparemeter space of much higher dimension. We need to be careful with the so-called 'curse of dimensionality' here - work out how many random forests you'll be comparing in your search, and remember that random forests take up a huge amount of memory!\n",
    "\n",
    "We'll implement this cross validation using GridSearchCV. First we create a dictionary to store the hyperparameters that we want to search over. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters =   {'n_estimators': [5,10,15,20,50,100],\n",
    "                'max_features': ['sqrt', 'auto', 'log2'],\n",
    "                'max_depth': [10, 30, 50, None],\n",
    "                'bootstrap': [True, False]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can specify our machine learning model as normal, then run the gridsearch. This is going to create $5\\times3\\times2\\times2 = 60$ random forests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "gridsearch = GridSearchCV(RandomForestRegressor(random_state=42), parameters, scoring='neg_mean_squared_error', cv=5, error_score=0)\n",
    "gridsearch.fit(X_train, y_train)\n",
    "\n",
    "best = gridsearch.best_params_\n",
    "\n",
    "rfr = RandomForestRegressor(n_estimators = best['n_estimators'], max_features = best['max_features'], max_depth = best['max_depth'], bootstrap = best['bootstrap'], random_state = 42)\n",
    "\n",
    "rfr.fit(X_train, y_train)\n",
    "y_pred = rfr.predict(X_test)\n",
    "mean_squared_error(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the gridsearch is finished, we can identify the best combination of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge: Handwriting recognition\n",
    "\n",
    "Train a random forest to recognise the digits 0-9. Download the data using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "mnist = fetch_mldata('MNIST original') \n",
    "X = mnist.data \n",
    "y = mnist.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$X$ is 70000 flattened 28 x 28 arrays of pixels from photographs of handwritten digits. $y$ contains their intended numerical values. For the arrays in $X$, a value of 0 corresponds to the darkest possible pixel (black), and a value of 1 corresponds to the lightest possible pixel (white). \n",
    "\n",
    "Good luck! You should be able to get >95% accuracy. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
